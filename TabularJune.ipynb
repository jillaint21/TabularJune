{"metadata":{"kernelspec":{"name":"python388jvsc74a57bd0199cfc071ad9d5fec8e9a7d21e2e3db19583d886eeaf77e0f714c2eaa07895cc","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["## Setup the environment"],"metadata":{}},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import seaborn as sns\n","import matplotlib.pyplot as  plt\n","# static images of your plot embedded in the notebook\n","%matplotlib inline  \n","\n","\n","from pandas import read_csv\n","from sklearn.model_selection import train_test_split\n","# from sklearn.preprocessing import LabelEncoder\n","# from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import mutual_info_classif\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","\n"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading data\n","\n"],"metadata":{}},{"cell_type":"code","source":["X = pd.read_csv(\"train.csv\", index_col=0)\n","X_test = pd.read_csv(\"test.csv\", index_col=0)\n","\n","print('Training data shape: ' + str(X.shape))\n","print('Testing data shape: ' + str(X_test.shape))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.head()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test.head()"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rstr(X, pred):\n","    rows = X.shape[0]\n","    types = X.dtypes\n","    counts = X.apply(lambda x: x.count())\n","    uniques = X.apply(lambda x: x.unique())\n","    uniques_count = X.apply(lambda x: x.unique().shape[0])\n","    nulls = X.apply(lambda x: x.isnull().sum())\n","    missing_rate = (X.isnull().sum()/ rows) * 100\n","    skewness = X.skew()\n","    kurtosis = X.kurt() \n","    \n","    if pred is None:\n","        cols = ['types', 'counts', 'nulls', 'missing rate', 'unique value count', 'unique value', 'skewness', 'kurtosis']\n","        values = pd.concat([types, counts, nulls, missing_rate, uniques_count, uniques, skewness, kurtosis], axis = 1)\n","    else:\n","        cols = ['types', 'counts', 'nulls', 'missing_rate', 'unique value count', 'unique value', 'skewness', 'kurtosis', 'corr '  + pred]\n","        values = pd.concat([types, counts, nulls, missing_rate, uniques_count, uniques, skewness, kurtosis, X.corr()[pred]], axis = 1, sort=False)\n","        \n","    values.columns = cols\n","    dtypes = values.types.value_counts()\n","    print('___________________________\\nData types:\\n',  values.types.value_counts())\n","    print('___________________________')\n","    return values\n","\n","details = rstr(X, None)\n","display(details.sort_values(by='missing rate', ascending=False))"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","plt.xticks(rotation=90)\n","sns.scatterplot(x=X.columns[0:75], y=X.skew())"]},{"source":["### Normal skewness is between -1 and 1, so all features are highly skewed"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","plt.xticks(rotation=90)\n","sns.scatterplot(x=X.columns[0:75], y=X.kurtosis())"]},{"source":["### Normal kurtosis range is <3 Need to normalize the data"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["details = rstr(X_test, None)\n","display(details)"],"metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"source":["## Minimal data processing before building baseline modles\n","\n","### Splitting test data for training and validation\n","Since this is a classification problem, let's make sure each class has reasonable number of samples in the train/validation split by using stratified train-test split.\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.groupby(by='target').size()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = X.pop('target')\n","X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"]},{"source":["print(X_train.shape, X_validate.shape, y_train.shape, y_validate.shape)"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check the ratio of samples in each class to make sure the split if reasonable\n","temp = pd.concat([X_train, y_train], axis=1)\n","temp.groupby(by='target').size()"]},{"source":["### Encode the target\n","\n","A simple label encoding for the train data."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the class label's last digit as encoding\n","y_train_enc = y_train.apply(lambda x: str(x)[-1])\n","print(y_train)\n","print(y_train_enc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use oneHot encoding for the validation target because the evaluation requirement of the competition.\n","y_validate_enc = pd.get_dummies(y_validate)\n","print(y_validate_enc)"]},{"source":["### Define scoring function based on competition documentation\n","\n","    log loss=−1N∑i=1N∑j=1Myijlog(pij)\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# log loss=−1N∑i=1N∑j=1Myijlog(pij)\n","# each row is divided by the row sum\n","# In order to avoid the extremes of the log function, predicted probabilities are replaced with max(min(p,1−10−15),10−15)\n","def get_logloss(prediction):\n","    prediction = pd.concat([pd.DataFrame(X_validate.index), pd.DataFrame(prediction)], axis=1).set_index('id')\n","    prediction.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']\n","\n","    prediction_prelog = prediction.applymap(lambda x: max(min(x, 1-10**-15),10**-15))\n","    prediction_scaled = prediction_prelog/prediction_prelog.sum(axis=1)[:,None]\n","    prediction_log = prediction_scaled.applymap(lambda x: np.log(x))\n","\n","    logloss = -prediction_log.multiply(y_validate_enc).values.sum()/result.shape[0]\n","\n","\n","    return logloss"]},{"source":["## Select three models to build a baseline\n","\n","*Decision Tree*\n","\n","*Naive Bayes*\n","\n","*Gradient Boost*\n","\n"],"cell_type":"markdown","metadata":{}},{"source":["### Decision Tree"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","model = DecisionTreeClassifier(random_state=0, min_samples_leaf=600)\n","model.fit(X_train, y_train_enc)\n","dt_prediction = model.predict_proba(X_validate)\n","print(dt_prediction)"]},{"source":["get_logloss(dt_prediction)\n","\n","# 10.43762174509599 \n","# First result with first model - Decision Tree(sample leaf set to 10)\n","\n","# 1.9677494343476694\n","# Changed sample leaf to 100\n","\n","# 1.8276959878207062\n","# Changed sample leaf to 200\n","\n","# 1.8107869385833477\n","# Changed sample leaf to 300\n","\n","# 1.8011824957234837\n","# Changed sample leaf to 400\n","\n","# 1.7999674006491755\n","# sample leaf 500\n","\n","# 1.798511069434078\n","# sample leaf 600\n","\n","# 1.79997749164544\n","# sample leaf 700      <------------- improvement stopped  "],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":["### Naive Bayes\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training a Naive Bayes classifier\n","from sklearn.naive_bayes import GaussianNB\n","gnb = GaussianNB()\n","gnb_prediction = GaussianNB().fit(X_train, y_train_enc).predict_proba(X_validate)\n","print(gnb_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(gnb_prediction)\n","\n","# 13.232232141881067\n","# first score for GaussianNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train_enc)\n","mnb_prediction = clf.predict_proba(X_validate)\n","print(mnb_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(mnb_prediction)\n","\n","# 4.75101374812709\n","# First score for MultinomialNB"]},{"source":["### Gradient Boosting"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10, random_state=0).fit(X_train, y_train_enc)\n","gb_prediction = clf.predict_proba(X_validate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(gb_prediction)\n","\n","# 4.131055844907932\n","# first score with n_estimators=100, learning_rate=1.0, max_depth=10"]},{"source":["================================================================================================================================================="],"cell_type":"markdown","metadata":{}},{"cell_type":"markdown","source":["## Data Engineering\n","\n","### Skewness and Kurtosis\n","According to our overview after loading the data, all features are highly skewed with high kurtosis. Let's address these first.\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.hist(grid=False, figsize=(30, 20), bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.agg(['skew', 'kurtosis']).transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import boxcox\n","\n","# Box-Cox Transformation in Python\n","# add a tiny number to each value to remove 0 since neither log or boxcox works with 0.\n","X_normalization_temp = X + 1\n","for col in X_normalization_temp.columns:\n","    X_normalization_temp[col] = boxcox(X_normalization_temp[col])[0]\n","\n","X_normalization_temp.hist(grid=False, figsize=(30, 20), bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_normalization = X_train + 1\n","for col in X_train_normalization.columns:\n","    X_train_normalization[col] = boxcox(X_train_normalization[col])[0]\n","\n","X_train_normalization.hist(grid=False, figsize=(30, 20), bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_validate_normalization = X_validate + 1\n","for col in X_validate_normalization.columns:\n","    X_validate_normalization[col] = boxcox(X_validate_normalization[col])[0]\n","\n","X_validate_normalization.hist(grid=False, figsize=(30, 20), bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_test_normalization = X_test + 1\n","for col in X_test_normalization.columns:\n","    X_test_normalization[col] = boxcox(X_test_normalization[col])[0]\n","\n","X_test_normalization.hist(grid=False, figsize=(30, 20), bins=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dt_model = DecisionTreeClassifier(random_state=0, min_samples_leaf=600)\n","dt_model.fit(X_train_normalization, y_train_enc)\n","dt_prediction = model.predict_proba(X_validate_normalization)\n","print(dt_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(dt_prediction)\n","\n","# 1.813381650885666\n","# This is not a better score compare to pre-normalization which makes sense since decision tree models don't require data normalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gnb = GaussianNB()\n","gnb_prediction = GaussianNB().fit(X_train_normalization, y_train_enc).predict_proba(X_validate_normalization)\n","print(gnb_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(gnb_prediction)\n","\n","# 5.2309914125829176\n","# The GaussianNB model has a big improvement (compare to 13.232232141881067)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clf = MultinomialNB()\n","clf.fit(X_train_normalization, y_train_enc)\n","mnb_prediction = clf.predict_proba(X_validate_normalization)\n","print(mnb_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_logloss(mnb_prediction)\n","\n","# 1.784880528011906\n","# the multinomialNB also has big improvement after normalization (compare to 4.75101374812709)"]},{"source":["This is added for source control testing\n"],"cell_type":"markdown","metadata":{}}]}